{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "33a2fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision\n",
    "from torchvision import transforms as tvt\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import parallel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import random\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import pathlib\n",
    "import random\n",
    "import skimage\n",
    "import cv2\n",
    "from torchvision import ops\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c017dcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "#try to understand transpose convolution:\n",
    "\n",
    "Tconv = nn.ConvTranspose2d(4, 2, 4, 1, 0)\n",
    "conv = nn.Conv2d(2, 1, 1, 1, 0)\n",
    "\n",
    "#Ask TA about this:\n",
    "# input_tensor = torch.tensor([[[0, 0.], \n",
    "#                             [ 1., 0.]],\n",
    "#                             [[0, 0.],\n",
    "#                             [1., 0.]]])\n",
    "# print(input_tensor.shape)\n",
    "# input_tensor = input_tensor.type(torch.float32)\n",
    "# print(input_tensor.dtype)\n",
    "# print(conv.weight.dtype, \"weight type\")\n",
    "# #conv.weight = conv.weight.type(torch.)\n",
    "# print(conv.weight)\n",
    "\n",
    "# #print(Tconv.weight)\n",
    "# print(conv(input_tensor))\n",
    "\n",
    "### End question.\n",
    "\n",
    "print(Tconv.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca202cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "conv1 = nn.Conv2d(3, 64, 3, 1, 0)\n",
    "print(conv1.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3afd0a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8213\n",
      "1000\n",
      "torch.Size([3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "### Create Dataloader\n",
    "\n",
    "root = '/Users/alim/Documents/ECE60146/hw7/'\n",
    "folders = ['pizzas/train/0', 'pizzas/eval']\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, folder):\n",
    "        super(MyDataset).__init__()\n",
    "        self.path = root + folder\n",
    "        self.imgs = os.listdir(self.path)\n",
    "\n",
    "        for img in self.imgs:\n",
    "            if (img == \"DS_Store\"):\n",
    "                self.imgs.remove(\".DS_Store\") #handle case when image isn't an image. Just remove it from the \n",
    "                #image list. \n",
    "                print('removed DS store')\n",
    "        self.to_Tensor_and_Norm = tvt.Compose([tvt.ToTensor(), tvt.Normalize([0], [1])])\n",
    "        print(len(self.imgs))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        PIL_img = Image.open(self.path + '/' + self.imgs[index])\n",
    "        torch_img = self.to_Tensor_and_Norm(PIL_img)\n",
    "        return  torch_img\n",
    "    \n",
    "train_dataset = MyDataset(root, folders[0])\n",
    "val_dataset = MyDataset(root, folders[1])\n",
    "        \n",
    "index = 2\n",
    "print(train_dataset[index].shape)\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, num_workers = 0, drop_last=False)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size = 40, num_workers = 0, drop_last = False)\n",
    "\n",
    "# for n, i in enumerate(my_train_dataloader):\n",
    "#     print(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8a8c8a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alim/Documents/ECE60146/hw7\n"
     ]
    }
   ],
   "source": [
    "### Discriminator and Generator Network class definitions. This is just taken from DL studio (for now!)\n",
    "print(os.getcwd())\n",
    "#####################################   Discriminator-Generator DG1   ######################################\n",
    "class DiscriminatorDG1(nn.Module):\n",
    "    \"\"\"\n",
    "    This is an implementation of the DCGAN Discriminator. I refer to the DCGAN network topology as\n",
    "    the 4-2-1 network.  Each layer of the Discriminator network carries out a strided\n",
    "    convolution with a 4x4 kernel, a 2x2 stride and a 1x1 padding for all but the final\n",
    "    layer. The output of the final convolutional layer is pushed through a sigmoid to yield\n",
    "    a scalar value as the final output for each image in a batch.\n",
    "\n",
    "    Class Path:  AdversarialLearning  ->   DataModeling  ->  DiscriminatorDG1\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorDG1, self).__init__()\n",
    "        self.conv_in = nn.Conv2d(  3,    64,      kernel_size=4,      stride=2,    padding=1)\n",
    "        self.conv_in2 = nn.Conv2d( 64,   128,     kernel_size=4,      stride=2,    padding=1)\n",
    "        self.conv_in3 = nn.Conv2d( 128,  256,     kernel_size=4,      stride=2,    padding=1)\n",
    "        self.conv_in4 = nn.Conv2d( 256,  512,     kernel_size=4,      stride=2,    padding=1)\n",
    "        self.conv_in5 = nn.Conv2d( 512,  1,       kernel_size=4,      stride=1,    padding=0)\n",
    "        self.bn1  = nn.BatchNorm2d(128)\n",
    "        self.bn2  = nn.BatchNorm2d(256)\n",
    "        self.bn3  = nn.BatchNorm2d(512)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    def forward(self, x):                 \n",
    "        x = torch.nn.functional.leaky_relu(self.conv_in(x), negative_slope=0.2, inplace=True)\n",
    "        x = self.bn1(self.conv_in2(x))\n",
    "        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2, inplace=True)\n",
    "        x = self.bn2(self.conv_in3(x))\n",
    "        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2, inplace=True)\n",
    "        x = self.bn3(self.conv_in4(x))\n",
    "        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2, inplace=True)\n",
    "        x = self.conv_in5(x)\n",
    "        x = self.sig(x)\n",
    "        return x\n",
    "\n",
    "class GeneratorDG1(nn.Module):\n",
    "    \"\"\"\n",
    "    This is an implementation of the DCGAN Generator. As was the case with the Discriminator network,\n",
    "    you again see the 4-2-1 topology here.  A Generator's job is to transform a random noise\n",
    "    vector into an image that is supposed to look like it came from the training dataset. (We refer \n",
    "    to the images constructed from noise vectors in this manner as fakes.)  As you will see later \n",
    "    in the \"run_gan_code()\" method, the starting noise vector is a 1x1 image with 100 channels.  In \n",
    "    order to output 64x64 output images, the network shown below use the Transpose Convolution \n",
    "    operator nn.ConvTranspose2d with a stride of 2.  If (H_in, W_in) are the height and the width \n",
    "    of the image at the input to a nn.ConvTranspose2d layer and (H_out, W_out) the same at the \n",
    "    output, the size pairs are related by\n",
    "                 H_out   =   (H_in - 1) * s   +   k   -   2 * p\n",
    "                 W_out   =   (W_in - 1) * s   +   k   -   2 * p\n",
    "\n",
    "    were s is the stride and k the size of the kernel.  (I am assuming square strides, kernels, and \n",
    "    padding). Therefore, each nn.ConvTranspose2d layer shown below doubles the size of the input.\n",
    "\n",
    "    Class Path:  AdversarialLearning  ->   DataModeling  ->  GeneratorDG1\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GeneratorDG1, self).__init__()\n",
    "        self.latent_to_image = nn.ConvTranspose2d( 100,   512,  kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.upsampler2 = nn.ConvTranspose2d( 512, 256, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.upsampler3 = nn.ConvTranspose2d (256, 128, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.upsampler4 = nn.ConvTranspose2d (128, 64,  kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.upsampler5 = nn.ConvTranspose2d(  64,  3,  kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.tanh  = nn.Tanh()\n",
    "    def forward(self, x):                     \n",
    "        x = self.latent_to_image(x)\n",
    "        x = torch.nn.functional.relu(self.bn1(x))\n",
    "        x = self.upsampler2(x)\n",
    "        x = torch.nn.functional.relu(self.bn2(x))\n",
    "        x = self.upsampler3(x)\n",
    "        x = torch.nn.functional.relu(self.bn3(x))\n",
    "        x = self.upsampler4(x)\n",
    "        x = torch.nn.functional.relu(self.bn4(x))\n",
    "        x = self.upsampler5(x)\n",
    "        x = self.tanh(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    " ##########################################   Critic-Generator CG2   ########################################\n",
    "class CriticCG2(nn.Module):\n",
    "    \"\"\"\n",
    "    For the sake of variety, the Critic implementation in CG2 as the same Marvin Cao's Discriminator:\n",
    "\n",
    "            https://github.com/caogang/wgan-gp\n",
    "\n",
    "    which in turn is the PyTorch version of the Tensorflow based Discriminator presented by the \n",
    "    authors of the paper \"Improved Training of Wasserstein GANs\" by Gulrajani, Ahmed, Arjovsky, Dumouli,\n",
    "    and Courville.\n",
    "\n",
    "    Class Path:  AdversarialLearning  ->   DataModeling  ->  CriticCG2\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CriticCG2, self).__init__()\n",
    "        self.DIM = 64\n",
    "        main = nn.Sequential(\n",
    "            nn.Conv2d(3, self.DIM, 5, stride=2, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.DIM, 2*self.DIM, 5, stride=2, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(2*self.DIM, 4*self.DIM, 5, stride=2, padding=2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.main = main\n",
    "        self.output = nn.Linear(4*4*4*self.DIM, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(-1, 3, 64, 64)\n",
    "        out = self.main(input)\n",
    "        out = out.view(-1, 4*4*4*self.DIM)\n",
    "        out = self.output(out)\n",
    "        out = out.mean(0)       \n",
    "        out = out.view(1)\n",
    "        return out\n",
    "\n",
    "class GeneratorCG2(nn.Module):\n",
    "    \"\"\"\n",
    "    The Generator code remains the same as for DG1 shown earlier.\n",
    "\n",
    "    Class Path:  AdversarialLearning  ->   DataModeling  ->  GeneratorCG2\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GeneratorCG2, self).__init__()\n",
    "        self.latent_to_image = nn.ConvTranspose2d( 100,   512,  kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.upsampler2 = nn.ConvTranspose2d( 512, 256, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.upsampler3 = nn.ConvTranspose2d (256, 128, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.upsampler4 = nn.ConvTranspose2d (128, 64,  kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.upsampler5 = nn.ConvTranspose2d(  64,  3,  kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.tanh  = nn.Tanh()\n",
    "    def forward(self, x):                   \n",
    "        x = self.latent_to_image(x)\n",
    "        x = torch.nn.functional.relu(self.bn1(x))\n",
    "        x = self.upsampler2(x)\n",
    "        x = torch.nn.functional.relu(self.bn2(x))\n",
    "        x = self.upsampler3(x)\n",
    "        x = torch.nn.functional.relu(self.bn3(x))\n",
    "        x = self.upsampler4(x)\n",
    "        x = torch.nn.functional.relu(self.bn4(x))\n",
    "        x = self.upsampler5(x)\n",
    "        x = self.tanh(x)\n",
    "        return x\n",
    "########################################   CG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "71f07c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "##  The training routines follow, first for a GAN constructed using either the DG1 and or the DG2 \n",
    "##  Discriminator-Generator Networks, and then for a WGAN constructed using either the CG1 or the CG2\n",
    "##  Critic-Generator Networks.\n",
    "############################################################################################################\n",
    "\n",
    "discriminator = DiscriminatorDG1()\n",
    "generator = GeneratorDG1()\n",
    "batch_size = 32\n",
    "learning_rate = 0.00001\n",
    "epochs = 1\n",
    "adversarial_beta1 = 0.5\n",
    "results_dir = '/Users/alim/Documents/ECE60146/hw7/results_DG1/'\n",
    "\n",
    "class MiniDLStudio(nn.Module):\n",
    "    def __init__(self, epochs, use_gpu, train_dataloader, batch_size, lr, LAMBDA):\n",
    "        super(MiniDLStudio, self).__init__()\n",
    "        self.epochs = 10\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = lr\n",
    "        self.LAMBDA = LAMBDA\n",
    "        if use_gpu is not None:\n",
    "            self.use_gpu = use_gpu\n",
    "            if use_gpu is True:\n",
    "                if torch.cuda.is_available():\n",
    "                    self.device = torch.device(\"cuda:0\")\n",
    "                else:\n",
    "                    raise Exception(\"You requested GPU support, but there's no GPU on this machine\")\n",
    "            else:\n",
    "                self.device = torch.device(\"cpu\")\n",
    "                print(\"assigned self.device\")\n",
    "\n",
    "    def weights_init(self,m):        \n",
    "        \"\"\"\n",
    "        Uses the DCGAN initializations for the weights\n",
    "        \"\"\"\n",
    "        classname = m.__class__.__name__     \n",
    "        if classname.find('Conv') != -1:         \n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)      \n",
    "        elif classname.find('BatchNorm') != -1:         \n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)       \n",
    "            nn.init.constant_(m.bias.data, 0)  \n",
    "            \n",
    "    def calc_gradient_penalty(self, netC, real_data, fake_data):\n",
    "        \"\"\"\n",
    "        Implementation by Marvin Cao: https://github.com/caogang/wgan-gp\n",
    "        Marvin Cao's code is a PyTorch version of the Tensorflow based implementation provided by\n",
    "        the authors of the paper \"Improved Training of Wasserstein GANs\" by Gulrajani, Ahmed, \n",
    "        Arjovsky, Dumouli,  and Courville.\n",
    "        \"\"\"\n",
    "        BATCH_SIZE = self.batch_size\n",
    "        LAMBDA = self.LAMBDA\n",
    "        epsilon = torch.rand(1).cpu() #may need to change to .cuda() when using GPU\n",
    "        interpolates = epsilon * real_data + ((1 - epsilon) * fake_data)\n",
    "        interpolates = interpolates.requires_grad_(True).cpu() #may need to change to .cuda() on GPU\n",
    "        critic_interpolates = netC(interpolates)\n",
    "        gradients = torch.autograd.grad(outputs=critic_interpolates, inputs=interpolates,\n",
    "                                  grad_outputs=torch.ones(critic_interpolates.size()).cpu(), #may need to change to .cuda() later! \n",
    "                                  create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "        return gradient_penalty\n",
    "\n",
    "def run_gan_code(self, discriminator, generator, results_dir):\n",
    "    \"\"\"\n",
    "    This function is meant for training a Discriminator-Generator based Adversarial Network.  \n",
    "    The implementation shown uses several programming constructs from the \"official\" DCGAN \n",
    "    implementations at the PyTorch website and at GitHub. \n",
    "\n",
    "    Regarding how to set the parameters of this method, see the following script\n",
    "\n",
    "                 dcgan_DG1.py\n",
    "\n",
    "    in the \"ExamplesAdversarialLearning\" directory of the distribution.\n",
    "    \"\"\"\n",
    "    dir_name_for_results = results_dir\n",
    "#     if os.path.exists(dir_name_for_results):\n",
    "#         files = glob.glob(dir_name_for_results + \"/*\")\n",
    "#         for file in files:\n",
    "#             if os.path.isfile(file):\n",
    "#                 os.remove(file)\n",
    "#             else:\n",
    "#                 files = glob.glob(file + \"/*\")\n",
    "#                 list(map(lambda x: os.remove(x), files))\n",
    "#     else:\n",
    "#         os.mkdir(dir_name_for_results)\n",
    "    #  Set the number of channels for the 1x1 input noise vectors for the Generator:\n",
    "    nz = 100\n",
    "    netD = discriminator.to(self.device)\n",
    "    netG = generator.to(self.device)\n",
    "    #  Initialize the parameters of the Discriminator and the Generator networks according to the\n",
    "    #  definition of the \"weights_init()\" method:\n",
    "    netD.apply(self.weights_init)\n",
    "    netG.apply(self.weights_init)\n",
    "    #  We will use a the same noise batch to periodically check on the progress made for the Generator:\n",
    "    fixed_noise = torch.randn(batch_size, nz, 1, 1, device=self.device)          \n",
    "    #  Establish convention for real and fake labels during training\n",
    "    real_label = 1   \n",
    "    fake_label = 0         \n",
    "    #  Adam optimizers for the Discriminator and the Generator:\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=learning_rate, betas=(adversarial_beta1, 0.999))    \n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=learning_rate, betas=(adversarial_beta1, 0.999))\n",
    "    #  Establish the criterion for measuring the loss at the output of the Discriminator network:\n",
    "    criterion = nn.BCELoss()\n",
    "    #  We will use these lists to store the results accumulated during training:\n",
    "    img_list = []                               \n",
    "    G_losses = []                               \n",
    "    D_losses = []                               \n",
    "    iters = 0                                   \n",
    "    print(\"\\n\\nStarting Training Loop...\\n\\n\")      \n",
    "    start_time = time.perf_counter()            \n",
    "    for epoch in range(epochs):        \n",
    "        g_losses_per_print_cycle = []           \n",
    "        d_losses_per_print_cycle = []           \n",
    "        # For each batch in the dataloader\n",
    "        for i, data in enumerate(self.train_dataloader, 0):         \n",
    "            #print(\"starting new batch\")\n",
    "            ##  Maximization Part of the Min-Max Objective of Eq. (3):\n",
    "            ##\n",
    "            ##  As indicated by Eq. (3) in the DCGAN part of the doc section at the beginning of this \n",
    "            ##  file, the GAN training boils down to carrying out a min-max optimization. Each iterative \n",
    "            ##  step of the max part results in updating the Discriminator parameters and each iterative \n",
    "            ##  step of the min part results in the updating of the Generator parameters.  For each \n",
    "            ##  batch of the training data, we first do max and then do min.  Since the max operation \n",
    "            ##  affects both terms of the criterion shown in the doc section, it has two parts: In the\n",
    "            ##  first part we apply the Discriminator to the training images using 1.0 as the target; \n",
    "            ##  and, in the second part, we supply to the Discriminator the output of the Generator \n",
    "            ##  and use 0 as the target. In what follows, the Discriminator is being applied to \n",
    "            ##  the training images:\n",
    "            netD.zero_grad()    \n",
    "            #real_images_in_batch = data[0].to(self.device)\n",
    "            real_images_in_batch = data.to(self.device)\n",
    "            #print(\"real_images_in_batch is:\", real_images_in_batch.shape)\n",
    "            #print(\"data shape is\", data.shape)\n",
    "            #  Need to know how many images we pulled in since at the tailend of the dataset, the \n",
    "            #  number of images may not equal the user-specified batch size:\n",
    "            b_size = real_images_in_batch.size(0)\n",
    "            #print(\"b_size is\", b_size)\n",
    "            label = torch.full((b_size,), real_label, dtype=torch.float, device=self.device) \n",
    "            #print(\"label size\", label.shape)\n",
    "            #print(\"label is:\", label)\n",
    "            output = netD(real_images_in_batch).view(-1)  \n",
    "            lossD_for_reals = criterion(output, label)                                                   \n",
    "            lossD_for_reals.backward()                                                                   \n",
    "            ##  That brings us the second part of what it takes to carry out the max operation on the\n",
    "            ##  min-max criterion shown in Eq. (3) in the doc section at the beginning of this file.\n",
    "            ##  part calls for applying the Discriminator to the images produced by the Generator from \n",
    "            ##  noise:\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=self.device)    \n",
    "            fakes = netG(noise) \n",
    "            label.fill_(fake_label) \n",
    "            ##  The call to fakes.detach() in the next statement returns a copy of the 'fakes' tensor \n",
    "            ##  that does not exist in the computational graph. That is, the call shown below first \n",
    "            ##  makes a copy of the 'fakes' tensor and then removes it from the computational graph. \n",
    "            ##  The original 'fakes' tensor continues to remain in the computational graph.  This ploy \n",
    "            ##  ensures that a subsequent call to backward() in the 3rd statement below would only\n",
    "            ##  update the netD weights.\n",
    "            output = netD(fakes.detach()).view(-1)    \n",
    "            lossD_for_fakes = criterion(output, label)    \n",
    "            ##  At this point, we do not care if the following call also calculates the gradients\n",
    "            ##  wrt the Discriminator weights since we are going to next iteration with 'netD.zero_grad()':\n",
    "            lossD_for_fakes.backward()          \n",
    "            lossD = lossD_for_reals + lossD_for_fakes    \n",
    "            d_losses_per_print_cycle.append(lossD)  \n",
    "            ##  Only the Discriminator weights are incremented:\n",
    "            optimizerD.step()  \n",
    "\n",
    "            ##  Minimization Part of the Min-Max Objective of Eq. (3):\n",
    "            ##\n",
    "            ##  That brings to the min part of the max-min optimization described in Eq. (3) the doc \n",
    "            ##  section at the beginning of this file.  The min part requires that we minimize \n",
    "            ##  \"1 - D(G(z))\" which, since D is constrained to lie in the interval (0,1), requires that \n",
    "            ##  we maximize D(G(z)).  We accomplish that by applying the Discriminator to the output \n",
    "            ##  of the Generator and use 1 as the target for each image:\n",
    "            netG.zero_grad()   \n",
    "            label.fill_(real_label)  \n",
    "            output = netD(fakes).view(-1)   \n",
    "            lossG = criterion(output, label)          \n",
    "            g_losses_per_print_cycle.append(lossG) \n",
    "            lossG.backward()    \n",
    "            ##  Only the Generator parameters are incremented:\n",
    "            optimizerG.step()\n",
    "            \n",
    "            if i % 100 == 99:                                                                           \n",
    "                current_time = time.perf_counter()                                                      \n",
    "                elapsed_time = current_time - start_time                                                \n",
    "                mean_D_loss = torch.mean(torch.FloatTensor(d_losses_per_print_cycle))                   \n",
    "                mean_G_loss = torch.mean(torch.FloatTensor(g_losses_per_print_cycle))                   \n",
    "                print(\"[epoch=%d/%d   iter=%4d   elapsed_time=%5d secs]     mean_D_loss=%7.4f    mean_G_loss=%7.4f\" % \n",
    "                              ((epoch+1),epochs,(i+1),elapsed_time,mean_D_loss,mean_G_loss))   \n",
    "                d_losses_per_print_cycle = []                                                           \n",
    "                g_losses_per_print_cycle = []                                                           \n",
    "            G_losses.append(lossG.item())                                                                \n",
    "            D_losses.append(lossD.item())                                                                \n",
    "            if (iters % 500 == 0) or ((epoch == epochs-1) and (i == len(self.train_dataloader)-1)):   \n",
    "                with torch.no_grad():             \n",
    "                    fake = netG(fixed_noise).detach().cpu()  ## detach() removes the fake from comp. graph. \n",
    "                                                             ## for creating its CPU compatible version\n",
    "                img_list.append(torchvision.utils.make_grid(fake, padding=1, pad_value=1, normalize=True))\n",
    "            iters += 1              \n",
    "    #  At the end of training, make plots from the data in G_losses and D_losses:\n",
    "    plt.figure(figsize=(10,5))    \n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")    \n",
    "    plt.plot(G_losses,label=\"G\")    \n",
    "    plt.plot(D_losses,label=\"D\") \n",
    "    plt.xlabel(\"iterations\")   \n",
    "    plt.ylabel(\"Loss\")         \n",
    "    plt.legend()          \n",
    "    plt.savefig(dir_name_for_results + \"/gen_and_disc_loss_training.png\") \n",
    "    plt.show()    \n",
    "    #  Make an animated gif from the Generator output images stored in img_list:            \n",
    "    images = []           \n",
    "    for imgobj in img_list:  \n",
    "        img = tvtF.to_pil_image(imgobj)  \n",
    "        images.append(img) \n",
    "    imageio.mimsave(dir_name_for_results + \"/generation_animation.gif\", images, fps=5)\n",
    "    #  Make a side-by-side comparison of a batch-size sampling of real images drawn from the\n",
    "    #  training data and what the Generator is capable of producing at the end of training:\n",
    "    real_batch = next(iter(self.train_dataloader)) \n",
    "    real_batch = real_batch[0]\n",
    "    plt.figure(figsize=(15,15))  \n",
    "    plt.subplot(1,2,1)   \n",
    "    plt.axis(\"off\")   \n",
    "    plt.title(\"Real Images\")    \n",
    "    plt.imshow(np.transpose(torchvision.utils.make_grid(real_batch.to(self.device), \n",
    "                                           padding=1, pad_value=1, normalize=True).cpu(),(1,2,0)))  \n",
    "    plt.subplot(1,2,2)                                                                             \n",
    "    plt.axis(\"off\")                                                                                \n",
    "    plt.title(\"Fake Images\")                                                                       \n",
    "    plt.imshow(np.transpose(img_list[-1],(1,2,0)))                                                 \n",
    "    plt.savefig(dir_name_for_results + \"/real_vs_fake_images.png\")                                 \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "42db36f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### WGAN CODE ####\n",
    "#### TRAINING LOOP ####\n",
    "\n",
    "results_dir_wgan_gp = '/Users/alim/Documents/ECE60146/hw7/results_wgan_gp/'\n",
    "beta1 = 0.5\n",
    "LAMBDA = 10\n",
    "\n",
    "\n",
    "def run_wgan_with_gp_code(self, critic, generator, results_dir):\n",
    "            \"\"\"\n",
    "            This function is meant for training a CG2-based Critic-Generator WGAN. Regarding how \n",
    "            to set the parameters of this method, see the following script in the \n",
    "            \"ExamplesAdversarialLearning\" directory of the distribution:\n",
    "\n",
    "                         wgan_with_gp_CG2.py\n",
    "            \"\"\"\n",
    "            dir_name_for_results = results_dir_wgan_gp\n",
    "#             if os.path.exists(dir_name_for_results):\n",
    "#                 files = glob.glob(dir_name_for_results + \"/*\")\n",
    "#                 for file in files:\n",
    "#                     if os.path.isfile(file):\n",
    "#                         os.remove(file)\n",
    "#                     else:\n",
    "#                         files = glob.glob(file + \"/*\")\n",
    "#                         list(map(lambda x: os.remove(x), files))\n",
    "#             else:\n",
    "#                 os.mkdir(dir_name_for_results)\n",
    "            #  Set the number of channels for the 1x1 input noise vectors for the Generator:\n",
    "            nz = 100\n",
    "            netC = critic.to(self.device)\n",
    "            netG = generator.to(self.device)\n",
    "            #  Initialize the parameters of the Critic and the Generator networks according to the\n",
    "            #  definition of the \"weights_init()\" method:\n",
    "            netC.apply(self.weights_init)\n",
    "            netG.apply(self.weights_init)\n",
    "            #  We will use a the same noise batch to periodically check on the progress made for the Generator:\n",
    "            fixed_noise = torch.randn(self.batch_size, nz, 1, 1, device=self.device)          \n",
    "            #  These are for training the Critic, 'one' is for the part of the training with actual\n",
    "            #  training images, and 'minus_one' is for the part based on the images produced by the \n",
    "            #  Generator:\n",
    "            one = torch.FloatTensor([1]).to(self.device)\n",
    "            minus_one = torch.FloatTensor([-1]).to(self.device)\n",
    "            #  Adam optimizers for the Critic and the Generator:\n",
    "            optimizerC = optim.Adam(netC.parameters(), lr=self.learning_rate, betas=(beta1, 0.999))    \n",
    "            optimizerG = optim.Adam(netG.parameters(), lr=self.learning_rate, betas=(beta1, 0.999))\n",
    "            img_list = []                               \n",
    "            Gen_losses = []                               \n",
    "            Cri_losses = []                               \n",
    "            iters = 0                                   \n",
    "            gen_iterations = 0\n",
    "            start_time = time.perf_counter()            \n",
    "            dataloader = self.train_dataloader\n",
    "            # For each epoch\n",
    "            for epoch in range(self.epochs):        \n",
    "                data_iter = iter(dataloader)\n",
    "                i = 0\n",
    "                ncritic   = 5\n",
    "                #  In this version of WGAN training, we enforce the 1-Lipschitz condition on the function\n",
    "                #  being learned by the Critic by requiring that the partial derivatives of the output of\n",
    "                #  the Critic with respect to its input equal one in magnitude. This is referred as imposing\n",
    "                #  a Gradient Penalty on the learning by the Critic.  As in the previous training\n",
    "                #  function, we start by turning on the \"requires_grad\" property of the Critic parameters:\n",
    "                while i < len(dataloader):\n",
    "                    for p in netC.parameters():\n",
    "                        p.requires_grad = True          \n",
    "                    ic = 0\n",
    "                    while ic < ncritic and i < len(dataloader):\n",
    "                        ic += 1\n",
    "                        #  The first two parts of what it takes to train the Critic are the same as for\n",
    "                        #  a regular WGAN.  We want to train the Critic to recognize the training images and,\n",
    "                        #  at the same time, the Critic should try to not believe the output of the Generator.\n",
    "                        netC.zero_grad()                                                                            \n",
    "                        real_images_in_batch =  next(data_iter) # was data_iter.next() before\n",
    "                        i += 1\n",
    "                        real_images_in_batch =  real_images_in_batch[0].to(self.device)   \n",
    "                        #  Need to know how many images we pulled in since at the tailend of the dataset, the \n",
    "                        #  number of images may not equal the user-specified batch size:\n",
    "                        b_size = real_images_in_batch.size(0)   \n",
    "                        #  Note that a single scalar is produced for all the data in a batch.  \n",
    "                        critic_for_reals_mean = netC(real_images_in_batch)     ## this is a batch based mean\n",
    "                        #  The gradient target is 'minus_one'.  Note that the gradient here is one of output of \n",
    "                        #  the network with respect to the learnable parameters:\n",
    "                        critic_for_reals_mean.backward(minus_one)     \n",
    "                        #  The second part of Critic training requires that we apply the Critic to the images\n",
    "                        #  produced by the Generator from a fresh batch of input noise vectors.\n",
    "                        noise = torch.randn(b_size, nz, 1, 1, device=self.device)    \n",
    "                        fakes = netG(noise)          \n",
    "                        #  Again, a single number is produced for the whole batch:\n",
    "                        critic_for_fakes_mean = netC(fakes.detach())  ## detach() returns a copy of the 'fakes' tensor that has\n",
    "                                                                      ## been removed from the computational graph. This ensures\n",
    "                                                                      ## that a subsequent call to backward() will only update the Critic\n",
    "                        #  The gradient target is 'one'.  Note that the gradient here is one of output of \n",
    "                        #  the network with respect to the learnable parameters:\n",
    "                        critic_for_fakes_mean.backward(one)         \n",
    "                        #  For the third part of Critic training, we need to first estimate the Gradient Penalty\n",
    "                        #  of the function being learned by the Critics with respect to the input to the function.\n",
    "                        gradient_penalty = self.calc_gradient_penalty(netC, real_images_in_batch, fakes)\n",
    "                        gradient_penalty.backward()               \n",
    "                        loss_critic = critic_for_fakes_mean - critic_for_reals_mean + gradient_penalty\n",
    "                        wasser_dist = critic_for_reals_mean - critic_for_fakes_mean                \n",
    "                        #  Update the Critic\n",
    "                        optimizerC.step()   \n",
    "\n",
    "                    #  That brings us to the training of the Generator.  First we must turn off the \"requires_grad\"\n",
    "                    #  of the Critic parameters since the Critic and the Generator are to be updated independently:\n",
    "                    for p in netC.parameters():\n",
    "                        p.requires_grad = False\n",
    "                    netG.zero_grad()                         \n",
    "                    #  This is again a single scalar based characterization of the whole batch of the Generator images:\n",
    "                    noise = torch.randn(b_size, nz, 1, 1, device=self.device)    \n",
    "                    fakes = netG(noise)          \n",
    "                    critic_for_fakes_mean = netC(fakes)\n",
    "                    loss_gen = critic_for_fakes_mean\n",
    "                    #  The gradient target is 'minus_one'.  Note that the gradient here is one of output of the network\n",
    "                    #  with respect to the learnable parameters:\n",
    "                    loss_gen.backward(minus_one)      \n",
    "                    #  Update the Generator\n",
    "                    optimizerG.step()                                                                          \n",
    "                    gen_iterations += 1\n",
    "                    if i % (ncritic * 20) == 0:   \n",
    "                        current_time = time.perf_counter()                                                            \n",
    "                        elapsed_time = current_time - start_time                                                      \n",
    "                        print(\"[epoch=%d/%d   i=%4d   el_time=%5d secs]     loss_critic=%7.4f   loss_gen=%7.4f   Wasserstein_dist=%7.4f\" %  (epoch+1,self.epochs,i,elapsed_time,loss_critic.data[0], loss_gen.data[0], wasser_dist.data[0]))\n",
    "                    Gen_losses.append(loss_gen.data[0].item())      \n",
    "                    Cri_losses.append(loss_critic.data[0].item())   \n",
    "                    #  Get G's output on fixed_noise for the GIF animation:\n",
    "                    if (iters % 500 == 0) or ((epoch == self.epochs-1) and (i == len(self.train_dataloader)-1)): \n",
    "                        with torch.no_grad():                                                                        \n",
    "                            fake = netG(fixed_noise).detach().cpu()  ## detach() removes the fake from comp. graph\n",
    "                                                                     ## in order to produce a CPU compatible tensor\n",
    "                        img_list.append(torchvision.utils.make_grid(fake, padding=1, pad_value=1, normalize=True))   \n",
    "                    iters += 1                                                                                        \n",
    "            \n",
    "            #  At the end of training, make plots from the data in Gen_losses and Cri_losses:\n",
    "            plt.figure(figsize=(10,5))                                                                             \n",
    "            plt.title(\"Generator and Critic Loss During Training\")                                          \n",
    "            plt.plot(Gen_losses,label=\"G\")                                                                           \n",
    "            plt.plot(Cri_losses,label=\"C\")                                                                           \n",
    "            plt.xlabel(\"iterations\")                                                                               \n",
    "            plt.ylabel(\"Loss\")                                                                                     \n",
    "            plt.legend()                                                                                           \n",
    "            plt.savefig(dir_name_for_results + \"/gen_and_critic_loss_training.png\")                                  \n",
    "            plt.show()                                                                                             \n",
    "            #  Make an animated gif from the Generator output images stored in img_list:            \n",
    "            images = []                                                                                            \n",
    "            for imgobj in img_list:                                                                                \n",
    "                img = tvtF.to_pil_image(imgobj)                                                                    \n",
    "                images.append(img)                                                                                 \n",
    "            imageio.mimsave(dir_name_for_results + \"/generation_animation.gif\", images, fps=5)                     \n",
    "            \n",
    "            #  Make a side-by-side comparison of a batch-size sampling of real images drawn from the\n",
    "            #  training data and what the Generator is capable of producing at the end of training:\n",
    "            real_batch = next(iter(self.train_dataloader))                                                        \n",
    "            real_batch = real_batch[0]\n",
    "            plt.figure(figsize=(15,15))                                                                           \n",
    "            plt.subplot(1,2,1)                                                                                    \n",
    "            plt.axis(\"off\")                                                                                       \n",
    "            plt.title(\"Real Images\")                                                                              \n",
    "            plt.imshow(np.transpose(torchvision.utils.make_grid(real_batch.to(self.device), \n",
    "                                               padding=1, pad_value=1, normalize=True).cpu(),(1,2,0)))  \n",
    "            plt.subplot(1,2,2)                                                                             \n",
    "            plt.axis(\"off\")                                                                                \n",
    "            plt.title(\"Fake Images\")                                                                       \n",
    "            plt.imshow(np.transpose(img_list[-1],(1,2,0)))                                                 \n",
    "            plt.savefig(dir_name_for_results + \"/real_vs_fake_images.png\")                                 \n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "082fa3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assigned self.device\n",
      "assigned self.device\n",
      "[epoch=1/10   i= 100   el_time=   56 secs]     loss_critic= 8.7762   loss_gen=-0.2094   Wasserstein_dist= 1.2202\n",
      "[epoch=1/10   i= 200   el_time=  111 secs]     loss_critic=-32.3095   loss_gen=-1.5150   Wasserstein_dist=42.2424\n",
      "[epoch=1/10   i= 300   el_time=  166 secs]     loss_critic=-145.2602   loss_gen=24.5447   Wasserstein_dist=154.8529\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[237], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#wgan.show_sample_images_from_dataset(dls)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m mdls_wgan \u001b[38;5;241m=\u001b[39m MiniDLStudio(epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, use_gpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, train_dataloader \u001b[38;5;241m=\u001b[39m train_dataloader, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     11\u001b[0m                         lr \u001b[38;5;241m=\u001b[39m learning_rate, LAMBDA \u001b[38;5;241m=\u001b[39m LAMBDA)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mrun_wgan_with_gp_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmdls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults_dir_wgan_gp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[235], line 98\u001b[0m, in \u001b[0;36mrun_wgan_with_gp_code\u001b[0;34m(self, critic, generator, results_dir)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m#  For the third part of Critic training, we need to first estimate the Gradient Penalty\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m#  of the function being learned by the Critics with respect to the input to the function.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m gradient_penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_gradient_penalty(netC, real_images_in_batch, fakes)\n\u001b[0;32m---> 98\u001b[0m \u001b[43mgradient_penalty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m               \n\u001b[1;32m     99\u001b[0m loss_critic \u001b[38;5;241m=\u001b[39m critic_for_fakes_mean \u001b[38;5;241m-\u001b[39m critic_for_reals_mean \u001b[38;5;241m+\u001b[39m gradient_penalty\n\u001b[1;32m    100\u001b[0m wasser_dist \u001b[38;5;241m=\u001b[39m critic_for_reals_mean \u001b[38;5;241m-\u001b[39m critic_for_fakes_mean                \n",
      "File \u001b[0;32m/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ece60146/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mdls = MiniDLStudio(epochs = 10, use_gpu = False, train_dataloader = train_dataloader, batch_size=batch_size,\n",
    "                   lr = learning_rate, LAMBDA = LAMBDA)\n",
    "# run_gan_code(mdls, discriminator, generator, results_dir)\n",
    "\n",
    "#run wgan code:\n",
    "critic = CriticCG2()\n",
    "generator = GeneratorCG2()\n",
    "\n",
    "#wgan.show_sample_images_from_dataset(dls)\n",
    "mdls_wgan = MiniDLStudio(epochs = 2, use_gpu = False, train_dataloader = train_dataloader, batch_size=batch_size,\n",
    "                        lr = learning_rate, LAMBDA = LAMBDA)\n",
    "run_wgan_with_gp_code(mdls, critic=critic, generator=generator, \n",
    "                           results_dir=results_dir_wgan_gp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6bd11be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alim/Documents/ECE60146/hw7\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "d = os.listdir('/Users/alim/Documents/ECE60146/hw7/pizzas/eval/')\n",
    "for n, i in enumerate(d):\n",
    "\n",
    "    #print(i)\n",
    "    PIL_img = Image.open('/Users/alim/Documents/ECE60146/hw7/pizzas/eval/' + i)\n",
    "    np_img = np.uint8(PIL_img)\n",
    "    #print(np_img.shape)\n",
    "    if np_img.shape != (64, 64, 3):\n",
    "        print(\"bad shp\")\n",
    "        print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ece60146]",
   "language": "python",
   "name": "conda-env-ece60146-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
